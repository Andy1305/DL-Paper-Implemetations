{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXwicGwC6CQa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision.transforms import transforms as trans\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trainable\n",
        "\n",
        "class Trainable:\n",
        "    def __init__(self, epochs, loss_f, batch_size, in_shape = (224, 224), has_sched = False, clip = False):\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_f = loss_f\n",
        "        self.has_sched = has_sched\n",
        "        self.clip = clip\n",
        "        self.transforms = trans.Compose([trans.ToTensor(), trans.Resize(in_shape)])\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self = self.to(self.device)\n",
        "\n",
        "    def fit(self, ds):\n",
        "        # TODO: set pin_memory = True\n",
        "        train_set = ds('/content/', train = True, transform = self.transforms, download = True)\n",
        "        train_loader = torch.utils.data.DataLoader(train_set, self.batch_size, shuffle = True)\n",
        "        \n",
        "        self.min_loss = 10000\n",
        "        for e in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            for x, y in train_loader:\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                out = self(x)\n",
        "                loss = self.loss_f(out, y)\n",
        "                total_loss += loss.item()\n",
        "                self.optim.zero_grad()\n",
        "                loss.backward()\n",
        "                if self.clip:\n",
        "                    nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
        "                self.optim.step()\n",
        "            if self.has_sched:\n",
        "                self.sched.step()\n",
        "            \n",
        "            print(\"Train loss #\", e, \":\", round(total_loss / len(train_set), 3))\n",
        "            self.validate(ds)\n",
        "\n",
        "    def validate(self, ds):\n",
        "        test_set = ds('/content/', train = True, transform = self.transforms, download = True)\n",
        "        test_loader = torch.utils.data.DataLoader(test_set, self.batch_size)\n",
        "        \n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                out = self(x)\n",
        "                loss = self.loss_f(out, y)\n",
        "                total_loss += loss.item()\n",
        "                pred = out.argmax(1)\n",
        "                correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "            if self.min_loss > total_loss:\n",
        "                self.min_loss = total_loss\n",
        "                torch.save({\n",
        "                    \"optim\": self.optim.state_dict(),\n",
        "                    \"model\": self.state_dict()\n",
        "                }, \"/content/\" + self.__class__.__name__ + \"_best_model.pth\")\n",
        "        print(\"Test loss:\", round(total_loss / len(test_set), 3), \"Accuracy:\", 100 * correct / len(test_set))\n",
        "        self.train()\n",
        "\n",
        "    def load_model(self):\n",
        "        self.load_state_dict(torch.load(\"/content/\" + self.__class__.__name__ + \"_best_model.pth\"))\n"
      ],
      "metadata": {
        "id": "d-8pDhBiM_os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title AlexNet\n",
        "## https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
        "\n",
        "class AlexNet(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels):\n",
        "        nn.Module.__init__(self)\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 96, (11, 11), 4),\n",
        "            nn.ReLU(),\n",
        "            #nn.LocalResponseNorm(5, 0.0001, 0.75, 2),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            nn.Conv2d(96, 256, (5, 5), padding = 2),\n",
        "            nn.ReLU(),\n",
        "            #nn.LocalResponseNorm(5, 0.0001, 0.75, 2),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            nn.Conv2d(256, 384, (3, 3), padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 384, (3, 3), padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, (3, 3), padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 5 * 5, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 10)\n",
        "        )\n",
        "        self.__init_weights()\n",
        "        Trainable.__init__(self, 100, nn.CrossEntropyLoss(reduction = 'sum'), 8, has_sched = True)\n",
        "\n",
        "    def __init_weights(self):\n",
        "        for layer in self.conv:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, 0, 0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "        #nn.init.constant_(self.conv[4].bias, 1)\n",
        "        #nn.init.constant_(self.conv[10].bias, 1)\n",
        "        #nn.init.constant_(self.conv[12].bias, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(x))\n",
        "\n",
        "    def fit(self, ds):\n",
        "        #self.optim = torch.optim.SGD(self.parameters(), 0.01, 0.9, 0.0005)\n",
        "        self.optim = torch.optim.Adam(self.parameters(), 0.0001)\n",
        "        self.sched = torch.optim.lr_scheduler.StepLR(self.optim, 30, gamma = 0.1)\n",
        "        Trainable.fit(self, ds)\n"
      ],
      "metadata": {
        "id": "GsycTipl9YUv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ZFNet\n",
        "# https://arxiv.org/pdf/1311.2901.pdf\n",
        "\n",
        "class ZFNet(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels):\n",
        "        nn.Module.__init__(self)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 96, 7, stride = 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            nn.LocalResponseNorm(5, 0.0001, 0.75, 2),\n",
        "            nn.Conv2d(96, 256, 5, stride = 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            nn.LocalResponseNorm(5, 0.0001, 0.75, 2),\n",
        "            nn.Conv2d(256, 384, 3),\n",
        "            nn.Conv2d(384, 384, 3),\n",
        "            nn.Conv2d(384, 256, 3),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 10)\n",
        "        )\n",
        "        Trainable.__init__(self, 100, nn.CrossEntropyLoss(), 128)\n",
        "        self.__init_weights()\n",
        "\n",
        "    def __init_weights(self):\n",
        "        for layer in self.conv:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.constant_(layer.weight, 0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "        nn.init.constant_(self.fc[1].weight, 0.01)\n",
        "        nn.init.constant_(self.fc[1].bias, 0)\n",
        "        nn.init.constant_(self.fc[3].weight, 0.01)\n",
        "        nn.init.constant_(self.fc[3].bias, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.fc(self.conv(input))\n",
        "\n",
        "    def fit(self, ds):\n",
        "        #self.optim = torch.optim.SGD(self.parameters(), 0.001, 0.2)\n",
        "        self.optim = torch.optim.Adam(self.parameters(), 0.0001)\n",
        "        Trainable.fit(self, ds)"
      ],
      "metadata": {
        "id": "0Yl2FGV3sxeu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VGG16\n",
        "# https://arxiv.org/pdf/1409.1556.pdf\n",
        "# https://arxiv.org/pdf/1505.06798.pdf\n",
        "\n",
        "class VGG16(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels):\n",
        "        nn.Module.__init__(self)\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(512, 512, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(7 * 7 * 512, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 10)\n",
        "        )\n",
        "        self.__init_weights()\n",
        "        Trainable.__init__(self, 100, nn.CrossEntropyLoss(), 16, has_sched = False)\n",
        "\n",
        "    def __init_weights(self):\n",
        "        for layer in self.conv:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, 0, 0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(x))\n",
        "\n",
        "    def fit(self, ds):\n",
        "        self.optim = torch.optim.Adam(self.parameters(), lr = 0.0001)\n",
        "        #self.optim = torch.optim.SGD(self.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 0.0005)\n",
        "        #self.sched = torch.optim.lr_scheduler.StepLR(self.optim, 30, gamma = 0.1)\n",
        "\n",
        "        Trainable.fit(self, ds)\n"
      ],
      "metadata": {
        "id": "6oPjhn7AeEgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ResNet50\n",
        "# https://arxiv.org/pdf/1512.03385.pdf\n",
        "# https://arxiv.org/pdf/1704.06904.pdf\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, nr_blocks, in_channels, out_channels, stride = 1):\n",
        "        nn.Module.__init__(self)\n",
        "        self.nr_blocks = nr_blocks\n",
        "        self.blocks = []\n",
        "        self.downsample = nn.Conv2d(in_channels, out_channels * 4, 1, stride = stride, bias = False)\n",
        "        for block in range(nr_blocks):\n",
        "            # starting the second block, update the in_channels and stride\n",
        "            if block == 1:\n",
        "                in_channels = out_channels * 4\n",
        "                stride = 1\n",
        "            self.blocks.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, 1, bias = False),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, 3, stride = stride, padding = 1, bias = False),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels * 4, 1, bias = False),\n",
        "                    nn.BatchNorm2d(out_channels * 4),\n",
        "                )\n",
        "            )\n",
        "        self.blocks = nn.Sequential(*self.blocks)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = torch.clone(x)\n",
        "        # downsample for the first block\n",
        "        identity = self.downsample(identity)\n",
        "        for block in range(self.nr_blocks):\n",
        "            x = self.blocks[block](x)\n",
        "            x += identity\n",
        "            x = nn.ReLU()(x)\n",
        "            identity = x\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet50(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels):\n",
        "        nn.Module.__init__(self)\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 7, stride = 2, padding = 3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2, padding = 1),\n",
        "\n",
        "            ConvBlock(3, 64, 64),\n",
        "            ConvBlock(4, 256, 128, stride = 2),\n",
        "            ConvBlock(6, 512, 256, stride = 2),\n",
        "            ConvBlock(3, 1024, 512, stride = 2),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 4, 10)\n",
        "        )\n",
        "        Trainable.__init__(self, 100, nn.CrossEntropyLoss(), 64)\n",
        "        self = self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(x))\n",
        "\n",
        "    def fit(self, ds):\n",
        "        #self.optim = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
        "        self.optim = torch.optim.SGD(self.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 0.0005)\n",
        "        #self.sched = torch.optim.lr_scheduler.StepLR(self.optim, 30, gamma = 0.1)\n",
        "        Trainable.fit(self, ds)"
      ],
      "metadata": {
        "id": "5GrsqtYRxBSv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GoogLeNet\n",
        "# https://arxiv.org/abs/1409.4842\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    # kwargs contains the number of filters for the convolutions in the\n",
        "    # same order as the table columns in the paper (1x1, 3x3 reduce, 3x3, 5x5 reduce, 5x5, pool proj)\n",
        "    def __init__(self, in_channels, kwargs):\n",
        "        super().__init__()\n",
        "        self.conv1x1_1 = ConvModule(in_channels, kwargs[0], 1)\n",
        "        self.conv3x3 = nn.Sequential(\n",
        "            ConvModule(in_channels, kwargs[1], 1),\n",
        "            ConvModule(kwargs[1], kwargs[2], 3, padding = 1)\n",
        "        )\n",
        "        self.conv5x5 = nn.Sequential(\n",
        "            ConvModule(in_channels, kwargs[3], 1),\n",
        "            ConvModule(kwargs[3], kwargs[4], 3, padding = 1)\n",
        "        )\n",
        "        self.conv1x1_2 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, 1, padding = 1),\n",
        "            ConvModule(in_channels, kwargs[5], 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1x1_1(x)\n",
        "        x2 = self.conv3x3(x)\n",
        "        x3 = self.conv5x5(x)\n",
        "        x4 = self.conv1x1_2(x)\n",
        "        return torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "\n",
        "class AuxiliaryOut(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(4),\n",
        "            ConvModule(in_channels, 128, 1),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.7),\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(self.conv(x).shape)\n",
        "        return self.fc(self.conv(x))\n",
        "        \n",
        "\n",
        "class GoogLeNet(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels):\n",
        "        nn.Module.__init__(self)\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.training_mode = True\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvModule(in_channels, 64, 7, stride = 2),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            #nn.LocalResponseNorm(),\n",
        "            ConvModule(64, 64, 1),\n",
        "            ConvModule(64, 192, 1),\n",
        "            #nn.LocalResponseNorm(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "        )\n",
        "        self.inception3a = InceptionModule(192, [64, 96, 128, 16, 32, 32])\n",
        "        self.inception3b = InceptionModule(256, [128, 128, 192, 32, 96, 64])\n",
        "        self.inception4a = InceptionModule(480, [192, 96, 208, 16, 48, 64])\n",
        "        self.aux_out_1 = AuxiliaryOut(512)\n",
        "        self.inception4b = InceptionModule(512, [160, 112, 224, 24, 64, 64])\n",
        "        self.inception4c = InceptionModule(512, [128, 128, 256, 24, 64, 64])\n",
        "        self.inception4d = InceptionModule(512, [112, 144, 288, 32, 64, 64])\n",
        "        self.aux_out_2 = AuxiliaryOut(528)\n",
        "        self.inception4e = InceptionModule(528, [256, 160, 320, 32, 128, 128])\n",
        "        self.inception5a = InceptionModule(832, [256, 160, 320, 32, 128, 128])\n",
        "        self.inception5b = InceptionModule(832, [384, 192, 384, 48, 128, 128])\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(1024, 10))\n",
        "\n",
        "        Trainable.__init__(self, 100, self.loss, 32)\n",
        "        self = self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "\n",
        "        x = self.inception4a(x)\n",
        "        if self.training_mode:\n",
        "            aux_out_1 = self.aux_out_1(x)\n",
        "        x = F.max_pool2d(x, 3, 2)\n",
        "        \n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "        if self.training_mode:\n",
        "            aux_out_2 = self.aux_out_2(x)\n",
        "        x = self.inception4e(x)\n",
        "        x = F.max_pool2d(x, 3, 2)\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = F.dropout(x, 0.4)\n",
        "        x = self.fc(x)\n",
        "        return x, aux_out_1, aux_out_2\n",
        "\n",
        "    def fit(self, ds):\n",
        "        self.training_mode = True\n",
        "        self.optim = torch.optim.SGD(self.parameters(), lr = 0.001, momentum = 0.9)\n",
        "        #self.sched = torch.optim.lr_scheduler.StepLR(self.optim, 8, gamma = 0.1)\n",
        "        Trainable.fit(self, ds)\n",
        "\n",
        "    def loss(self, out, y):\n",
        "        x, aux_out_1, aux_out_2 = out\n",
        "        l1 = F.cross_entropy(x, y)\n",
        "        l2 = F.cross_entropy(aux_out_1, y)\n",
        "        l3 = F.cross_entropy(aux_out_2, y)\n",
        "        return 0.3 * (l2 + l3) + l1\n",
        "\n"
      ],
      "metadata": {
        "id": "BgaqCrKAHcf0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inception-v3\n",
        "# https://arxiv.org/pdf/1512.00567.pdf\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "    def __init__(self, in_channels, pool1x1_filters):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv3x3 = nn.Sequential(\n",
        "           ConvModule(in_channels, 64, 1),\n",
        "           ConvModule(64, 96, 3, padding = 1),\n",
        "           ConvModule(96, 96, 3, padding = 1)\n",
        "        )\n",
        "        \n",
        "        self.conv5x5 = nn.Sequential(\n",
        "           ConvModule(in_channels, 48, 1),\n",
        "           #ConvModule(48, 96, 3, padding = 1),\n",
        "           #ConvModule(96, 96, 3, padding = 1),\n",
        "           ConvModule(48, 64, 5, padding = 2)\n",
        "        )\n",
        "\n",
        "        self.pool1x1 = nn.Sequential(\n",
        "           #nn.AvgPool2d(3),\n",
        "           ConvModule(in_channels, pool1x1_filters, 1)\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "           ConvModule(in_channels, 64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv3x3(x)\n",
        "        x2 = self.conv5x5(x)\n",
        "        x3 = self.pool1x1(x)\n",
        "        x4 = self.conv1x1(x)\n",
        "        return torch.cat([x1, x2, x3, x4], 1)\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "    # reduction module\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv3x3_3x3 = nn.Sequential(\n",
        "           ConvModule(in_channels, 64, 1),\n",
        "           ConvModule(64, 96, 3),\n",
        "           ConvModule(96, 96, 3, stride = 2, padding = 1)\n",
        "        )\n",
        "        \n",
        "        self.conv3x3 = ConvModule(in_channels, 384, 3, stride = 2)\n",
        "        self.pool = nn.MaxPool2d(3, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv3x3_3x3(x)\n",
        "        x2 = self.conv3x3(x)\n",
        "        x3 = self.pool(x)\n",
        "        return torch.cat([x1, x2, x3], 1)\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "    def __init__(self, in_channels, nr_7x7s):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv7x7_7x7 = nn.Sequential(\n",
        "           ConvModule(in_channels, nr_7x7s, 1),\n",
        "           ConvModule(nr_7x7s, nr_7x7s, (7, 1), padding = (3, 0)),\n",
        "           ConvModule(nr_7x7s, nr_7x7s, (1, 7), padding = (0, 3)),\n",
        "           ConvModule(nr_7x7s, nr_7x7s, (7, 1), padding = (3, 0)),\n",
        "           ConvModule(nr_7x7s, 192, (1, 7), padding = (0, 3))\n",
        "        )\n",
        "        \n",
        "        self.conv7x7 = nn.Sequential(\n",
        "           ConvModule(in_channels, nr_7x7s, 1),\n",
        "           ConvModule(nr_7x7s, nr_7x7s, (1, 7), padding = (0, 3)),\n",
        "           ConvModule(nr_7x7s, 192, (7, 1), padding = (3, 0))\n",
        "        )\n",
        "\n",
        "        self.pool1x1 = nn.Sequential(\n",
        "           #nn.AvgPool2d(3),\n",
        "           ConvModule(in_channels, 192, 1)\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = ConvModule(in_channels, 192, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv7x7_7x7(x)\n",
        "        x2 = self.conv7x7(x)\n",
        "        x3 = self.pool1x1(x)\n",
        "        x4 = self.conv1x1(x)\n",
        "        return torch.cat([x1, x2, x3, x4], 1)\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "    # reduction module\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv7x7 = nn.Sequential(\n",
        "            ConvModule(in_channels, 192, 1),\n",
        "            ConvModule(192, 192, (1, 7), padding = (0, 3)),\n",
        "            ConvModule(192, 192, (7, 1), padding = (3, 0)),\n",
        "            ConvModule(192, 192, 3, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.conv3x3 = nn.Sequential(\n",
        "           ConvModule(in_channels, 192, 1),\n",
        "           ConvModule(192, 320, 3, stride = 2)\n",
        "        )\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(3, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv7x7(x)\n",
        "        x2 = self.conv3x3(x)\n",
        "        x3 = self.pool(x)\n",
        "        return torch.cat([x1, x2, x3], 1)\n",
        "\n",
        "class InceptionE(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv3x3_3x3 = nn.Sequential(\n",
        "           ConvModule(in_channels, 448, 1),\n",
        "           ConvModule(448, 384, 3, padding = 1),\n",
        "           ConvModule(384, 384, (1, 3), padding = (0, 1)),\n",
        "           ConvModule(384, 384, (3, 1), padding = (1, 0))\n",
        "        )\n",
        "        \n",
        "        self.conv3x3 = nn.Sequential(\n",
        "           ConvModule(in_channels, 384, 1),\n",
        "           ConvModule(384, 384, (1, 3), padding = (0, 1)),\n",
        "           ConvModule(384, 384, (3, 1), padding = (1, 0))\n",
        "        )\n",
        "\n",
        "        self.pool1x1 = nn.Sequential(\n",
        "           #nn.AvgPool2d(3),\n",
        "           ConvModule(in_channels, 192, 1)\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = ConvModule(in_channels, 320, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv3x3_3x3[0](x)\n",
        "        x1 = self.conv3x3_3x3[1](x1)\n",
        "        x1_1x3 = self.conv3x3_3x3[2](x1)\n",
        "        x1_3x1 = self.conv3x3_3x3[3](x1)\n",
        "        x1 = torch.cat((x1_1x3, x1_3x1), 1)\n",
        "\n",
        "        x2 = self.conv3x3[0](x)\n",
        "        x2_1x3 = self.conv3x3[1](x2)\n",
        "        x2_3x1 = self.conv3x3[2](x2)\n",
        "        x2 = torch.cat((x2_1x3, x2_3x1), 1)\n",
        "\n",
        "        x3 = self.pool1x1(x)\n",
        "        x4 = self.conv1x1(x)\n",
        "        return torch.cat([x1, x2, x3, x4], 1)\n",
        "\n",
        "class AuxiliaryOut(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(5, 3),\n",
        "            ConvModule(in_channels, 128, 1),\n",
        "            ConvModule(128, 768, 5)\n",
        "        )\n",
        "        self.conv[2].stddev = 0.01\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(768, 10)\n",
        "        )\n",
        "        self.fc[2].stddev = 0.001\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(x))\n",
        "\n",
        "class InceptionV3(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels):\n",
        "        nn.Module.__init__(self)\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.training_mode = False\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvModule(in_channels, 32, 3, stride = 2),\n",
        "            ConvModule(32, 32, 3),\n",
        "            ConvModule(32, 64, 3, padding = 1),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            ConvModule(64, 80, 1),\n",
        "            ConvModule(80, 192, 3),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "        )\n",
        "        self.inception_a = nn.Sequential(\n",
        "            InceptionA(192, 32),\n",
        "            InceptionA(256, 64), \n",
        "            InceptionA(288, 64)\n",
        "        )\n",
        "        self.inception_b = InceptionB(288) # reduction\n",
        "        self.inception_c = nn.Sequential(\n",
        "            InceptionC(768, 128),\n",
        "            InceptionC(768, 160),\n",
        "            InceptionC(768, 160),\n",
        "            InceptionC(768, 192)\n",
        "        )\n",
        "\n",
        "        self.inception_d = InceptionD(768) # reduction\n",
        "        self.inception_e = nn.Sequential(\n",
        "            InceptionE(1280),\n",
        "            InceptionE(2048)\n",
        "        )\n",
        "        self.out_aux = AuxiliaryOut(768)\n",
        "        self.fc = nn.Sequential(nn.AvgPool2d(1), nn.Dropout(), nn.Flatten(), nn.Linear(2048, 10))\n",
        "        Trainable.__init__(self, 100, self.loss, 32, (299, 299), True, True)\n",
        "        self = self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.inception_a(x)\n",
        "        x = self.inception_b(x)\n",
        "        x = self.inception_c(x)\n",
        "        if self.training_mode:\n",
        "            x_out_aux = self.out_aux(x)\n",
        "        x = self.inception_d(x)\n",
        "        x = self.inception_e(x)\n",
        "        x = F.max_pool2d(x, 8)\n",
        "        return self.fc(x), x_out_aux\n",
        "\n",
        "    def fit(self, ds):\n",
        "        self.training_mode = True\n",
        "        #self.optim = torch.optim.SGD(self.parameters(), lr = 0.045, momentum = 0.9)\n",
        "        self.optim = torch.optim.RMSprop(self.parameters(), lr = 0.045, eps = 1.0, momentum = 0.9)\n",
        "        self.sched = torch.optim.lr_scheduler.StepLR(self.optim, 2, gamma = 0.94)\n",
        "        Trainable.fit(self, ds)\n",
        "\n",
        "    def loss(self, out, y):\n",
        "        x, aux_out = out\n",
        "        l1 = F.cross_entropy(x, y)\n",
        "        l2 = F.cross_entropy(aux_out, y)\n",
        "        return l1 + l2\n"
      ],
      "metadata": {
        "id": "SM6SYM4fZTK9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Xception\n",
        "# https://arxiv.org/abs/1610.02357\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class SeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel, groups = in_channels, **kwargs)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return self.bn(x)\n",
        "\n",
        "\n",
        "class EntryModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvModule(in_channels, 32, 3, stride = 2),\n",
        "            ConvModule(32, 64, 3),\n",
        "        )\n",
        "        \n",
        "        self.identity_1 = nn.Sequential(\n",
        "            ConvModule(64, 128, 1, stride = 2),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "        self.sep_conv_1 = nn.Sequential(\n",
        "            SeparableConv(64, 128, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(128, 128, 3, padding = 1),\n",
        "            nn.MaxPool2d(3, 2, 1),\n",
        "        )\n",
        "\n",
        "        self.identity_2 = nn.Sequential(\n",
        "            ConvModule(128, 256, 1, stride = 2),\n",
        "            nn.BatchNorm2d(256)\n",
        "        )\n",
        "        self.sep_conv_2 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(128, 256, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(256, 256, 3, padding = 1),\n",
        "            nn.MaxPool2d(3, 2, 1),\n",
        "        )\n",
        "\n",
        "        self.identity_3 = nn.Sequential(\n",
        "            ConvModule(256, 728, 1, stride = 2),\n",
        "            nn.BatchNorm2d(728)\n",
        "        )\n",
        "        self.sep_conv_3 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(256, 728, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(728, 728, 3, padding = 1),\n",
        "            nn.MaxPool2d(3, 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "\n",
        "        identity = self.identity_1(x)\n",
        "        x = self.sep_conv_1(x)\n",
        "        x += identity\n",
        "        \n",
        "        identity = self.identity_2(x)\n",
        "        x = self.sep_conv_2(x)\n",
        "        x += identity\n",
        "        \n",
        "        identity = self.identity_3(x)\n",
        "        x = self.sep_conv_3(x)\n",
        "        x += identity\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class MiddleModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(728, 728, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(728, 728, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(728, 728, 3, padding = 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.conv(x)\n",
        "        return x + identity\n",
        "\n",
        "class ExitModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_sep_1 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(728, 728, 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(728, 1024, 3, padding = 1),\n",
        "            nn.MaxPool2d(3, 2, 1)\n",
        "        )\n",
        "        self.identity = nn.Sequential(\n",
        "            ConvModule(728, 1024, 1, stride = 2),\n",
        "            nn.BatchNorm2d(1024)\n",
        "        )\n",
        "        self.conv_sep_2 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(1024, 1536, 3),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv(1536, 2048, 3),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.identity(x)\n",
        "        x = self.conv_sep_1(x)\n",
        "        x += identity\n",
        "        x = self.conv_sep_2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Xception(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels):\n",
        "        nn.Module.__init__(self)\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.conv = nn.Sequential(\n",
        "            EntryModule(in_channels),\n",
        "            MiddleModule(),\n",
        "            MiddleModule(),\n",
        "            MiddleModule(),\n",
        "            MiddleModule(),\n",
        "            MiddleModule(),\n",
        "            MiddleModule(),\n",
        "            MiddleModule(),\n",
        "            ExitModule()\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "        Trainable.__init__(self, 100, nn.CrossEntropyLoss(), 32)\n",
        "        self = self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(x))\n",
        "\n",
        "    def fit(self, ds):\n",
        "        self.optim = torch.optim.SGD(self.parameters(), lr = 0.045, momentum = 0.9, weight_decay = 0.00001)\n",
        "        #self.sched = torch.optim.lr_scheduler.StepLR(self.optim, 2, gamma = 0.94)\n",
        "        Trainable.fit(self, ds)\n"
      ],
      "metadata": {
        "id": "tVZ187TzAuJu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MobileNet\n",
        "## https://arxiv.org/pdf/1704.04861.pdf\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ConvDW(nn.Module):\n",
        "    # depthwise separable 2D convolution\n",
        "    def __init__(self, in_channels, out_channels, alpha = 1, **kwargs):\n",
        "        super().__init__()\n",
        "        in_channels *= alpha\n",
        "        out_channels *= alpha\n",
        "        self.depthwise = ConvModule(in_channels, in_channels, 3, padding = 1, groups = in_channels, **kwargs)\n",
        "        self.pointwise = ConvModule(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileNet(nn.Module, Trainable):\n",
        "    def __init__(self, in_channels, alpha = 1, ro = 1):\n",
        "        nn.Module.__init__(self)\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # alpha - hyperparamter changing the default number of kernels (alpha * nr_kernels)\n",
        "        # ro - hyperparameter changing the default image size (ro * H x ro * W)\n",
        "        self.conv = nn.Sequential(\n",
        "            ConvModule(in_channels, 32, 3, stride = 2, padding = 1),\n",
        "            ConvDW(32, 64),\n",
        "            ConvDW(64, 128, stride = 2),\n",
        "            ConvDW(128, 128),\n",
        "            ConvDW(128, 256, stride = 2),\n",
        "            ConvDW(256, 256),\n",
        "            ConvDW(256, 512, stride = 2),\n",
        "            ConvDW(512, 512),\n",
        "            ConvDW(512, 512),\n",
        "            ConvDW(512, 512),\n",
        "            ConvDW(512, 512),\n",
        "            ConvDW(512, 512),\n",
        "            ConvDW(512, 1024, stride = 2),\n",
        "            ConvDW(1024, 1024),\n",
        "            nn.AvgPool2d(7)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "        Trainable.__init__(self, 100, nn.CrossEntropyLoss(), 32)\n",
        "        self = self.to(self.device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv(x))\n",
        "\n",
        "    def fit(self, ds):\n",
        "        #self.optim = torch.optim.RMSprop(self.parameters(), lr = 0.1, momentum = 0.9)\n",
        "        self.optim = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
        "        #self.sched = torch.optim.lr_scheduler.StepLR(self.optim, 8, gamma = 0.1)\n",
        "        Trainable.fit(self, ds)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4wh3PXI1_0TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 3, 224, 224)\n",
        "net = VGG16(1)\n",
        "net.fit(MNIST)\n",
        "\n",
        "#TODO: VGG16"
      ],
      "metadata": {
        "id": "hFXgiJDYMLvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0e0c94-818b-4bb2-e516-e73ce628337b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss # 0 : 0.144\n",
            "Test loss: 0.144 Accuracy: 11.236666666666666\n",
            "Train loss # 1 : 0.144\n",
            "Test loss: 0.144 Accuracy: 11.236666666666666\n"
          ]
        }
      ]
    }
  ]
}